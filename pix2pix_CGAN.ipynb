{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7b1f7e9",
   "metadata": {},
   "source": [
    " ## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "import datetime\n",
    "import tqdm\n",
    "import scipy.io\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "# specify the gpu to be utilized\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f776cfd2",
   "metadata": {},
   "source": [
    "## Load the training and test data sets\n",
    "\n",
    "The data for each set consists of 909 images of 3D aortic anatomy, which were either obtained using CT angiography or 4D flow MRI. The peak systolic velocity for each set was also measured using 4D flow MRI. The anatomy and flow data have been pre-processed and are each sized at 192 x 64 x 64 pixels, representing the height, width, and slice dimensions, respectively.\n",
    "\n",
    "As described in the [pix2pix paper](https://arxiv.org/abs/1611.07004){:.external}, apply random jittering and mirroring to preprocess the training set.\n",
    "\n",
    "Define several functions that:\n",
    "\n",
    "1. Resize each `192 x 64` image to a larger height and widthâ€”`222 x 94`.\n",
    "2. Randomly crop it back to `192 x 64`.\n",
    "3. Randomly flip the image horizontally i.e. left to right (random mirroring).\n",
    "\n",
    "It should be noted that in this implementation we don't normalize the images to the `[-1, 1]` range, because we need to predict the exact value of aortic hemodynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b96381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the data size,  batch siz\n",
    "BATCH_SIZE = 1 # batch size of 1 has given the best results\n",
    "DATA_HEIGHT = 192\n",
    "DATA_WIDTH = 64\n",
    "DATA_DEPTH = 64\n",
    "\n",
    "# resize both anatomy and flow to a larger size to crop them later\n",
    "def resize(anatomy,flow,height,width):\n",
    "    anatomy = tf.image.resize(anatomy,[height,width],method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    flow = tf.image.resize(flow,[height,width],method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return anatomy,flow\n",
    "\n",
    "# randomly crop the enlarged data to its original size\n",
    "def random_crop(anatomy,flow):\n",
    "    stacked_data = tf.stack([tf.squeeze(anatomy),tf.squeeze(flow)],axis=0)\n",
    "    cropped_data = tf.image.random_crop(stacked_data,\n",
    "                                        size=[2,DATA_HEIGHT,DATA_WIDTH,DATA_DEPTH])\n",
    "    return tf.expand_dims(cropped_data[0],axis=-1),tf.expand_dims(cropped_data[1],axis=-1)\n",
    "\n",
    "# random jitter\n",
    "@tf.function()\n",
    "def random_jitter(anatomy,flow):\n",
    "    anatomy,flow = resize(anatomy,flow,DATA_HEIGHT+30,DATA_WIDTH+30) # resize\n",
    "    anatomy,flow = random_crop(anatomy,flow) # crop back to the original size\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        anatomy = tf.image.flip_left_right(anatomy)\n",
    "        flow = tf.image.flip_left_right(flow)\n",
    "    return anatomy,flow\n",
    "\n",
    "# crop the enlarged images back to original size around the center\n",
    "@tf.function\n",
    "def center_crop(image, size):\n",
    "    if not isinstance(size, (tuple, list)):\n",
    "        size = [size, ize]\n",
    "    offset_height = (tf.shape(image)[-3]-size[0])//2\n",
    "    offset_width = (tf.shape(image)[-2]-size[1])//2\n",
    "    return tf.image.crop_to_bounding_box(image,offset_height,offset_width,size[0],size[1])\n",
    "\n",
    "# the parser function for reading the training data set\n",
    "def parser_train(tfrecord):\n",
    "    feature = tf.io.parse_single_example(tfrecord,{'A': tf.io.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "              'B' : tf.io.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "              'height' : tf.io.FixedLenFeature(shape=[], dtype=tf.int64),\n",
    "              'width'  : tf.io.FixedLenFeature(shape=[], dtype=tf.int64),\n",
    "              'depth'  : tf.io.FixedLenFeature(shape=[], dtype=tf.int64)})\n",
    "    height = tf.cast(feature[\"height\"], tf.int32)\n",
    "    width  = tf.cast(feature[\"width\"], tf.int32)\n",
    "    depth  = tf.cast(feature[\"depth\"], tf.int32)\n",
    "    A = tf.io.decode_raw(feature['A'], tf.float32) \n",
    "    A = tf.reshape(A, [height, width, depth])\n",
    "    A = center_crop(A, [DATA_HEIGHT,DATA_WIDTH])\n",
    "    B = tf.io.decode_raw(feature['B'], tf.float32) \n",
    "    B = tf.reshape(B, [height, width, depth])\n",
    "    B = center_crop(B, [DATA_HEIGHT,DATA_WIDTH])\n",
    "    return random_jitter(A, B)\n",
    "\n",
    "# the parser function for reading the test dataset.Importantly, the test data will not undergo the random jittering\n",
    "def parser_test(tfrecord):\n",
    "    feature = tf.io.parse_single_example(tfrecord,{'A': tf.io.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "              'B' : tf.io.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "              'height' : tf.io.FixedLenFeature(shape=[], dtype=tf.int64),\n",
    "              'width'  : tf.io.FixedLenFeature(shape=[], dtype=tf.int64),\n",
    "              'depth'  : tf.io.FixedLenFeature(shape=[], dtype=tf.int64)})\n",
    "    height = tf.cast(feature[\"height\"], tf.int32)\n",
    "    width  = tf.cast(feature[\"width\"], tf.int32)\n",
    "    depth  = tf.cast(feature[\"depth\"], tf.int32)\n",
    "    A = tf.io.decode_raw(feature['A'], tf.float32) \n",
    "    A = tf.reshape(A, [height, width, depth])\n",
    "    A = center_crop(A, [DATA_HEIGHT,DATA_WIDTH])\n",
    "    B = tf.io.decode_raw(feature['B'], tf.float32) \n",
    "    B = tf.reshape(B, [height, width, depth])\n",
    "    B = center_crop(B, [DATA_HEIGHT,DATA_WIDTH])\n",
    "    return tf.expand_dims(A,axis=-1),tf.expand_dims(B,axis=-1)\n",
    "\n",
    "\n",
    "tfrecord_path = 'anatomay2flow_train_pix2pix.tfrecords' # specify the path to the training set\n",
    "dataset_train = tf.data.TFRecordDataset(tfrecord_path)\n",
    "DATA_SIZE = len(list(dataset_train)) # return the size of the training set\n",
    "BUFFER_SIZE = DATA_SIZE\n",
    "dataset_train = dataset_train.map(map_func=parser_train,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset_train = dataset_train.shuffle(buffer_size=BUFFER_SIZE)\n",
    "dataset_train = dataset_train.batch(BATCH_SIZE)\n",
    "\n",
    "# create a set of test samples\n",
    "tfrecord_path = 'anatomay2flow_test_pix2pix.tfrecords'\n",
    "dataset_test = tf.data.TFRecordDataset(tfrecord_path)\n",
    "dataset_test = dataset_test.map(map_func=parser_test,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset_test = dataset_test.batch(BATCH_SIZE)\n",
    "dataset_test = iter(dataset_test);\n",
    "\n",
    "# save ten samples in the test to monitor the model performance as it is being trained\n",
    "Asample = list()\n",
    "Bsample = list()\n",
    "ii = 1\n",
    "for a,b in dataset_test:\n",
    "    Asample.append(a)\n",
    "    Bsample.append(b)\n",
    "    ii+=1\n",
    "    if ii>10:\n",
    "        print(a.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e44934",
   "metadata": {},
   "source": [
    "## Build the generator\n",
    "Build the generator\n",
    "\n",
    "The generator is a modified U-Net{:.external}. A U-Net consists of an encoder (downsampler) and decoder (upsampler).\n",
    "\n",
    "Each block in the encoder is: Convolution -> Batch normalization -> Leaky ReLU\n",
    "\n",
    "Each block in the decoder is: Transposed convolution -> Batch normalization -> Dropout (applied to the first 3 blocks) -> ReLU\n",
    "\n",
    "There are skip connections between the encoder and decoder (as in the U-Net)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aef21c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the generator downsampler\n",
    "def downsample(filters,size,apply_batchnorm=True):\n",
    "    initializer=tf.random_normal_initializer(0,0.02)\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(tf.keras.layers.Conv3D(filters,size,strides=2,padding='same',\n",
    "                                    kernel_initializer=initializer,use_bias=False))\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "    return result\n",
    "\n",
    "# verify the shape of the downsampler output\n",
    "down_model = downsample(3,4)\n",
    "A = Asample[0]\n",
    "print(\"Shape of the inputs of a downsampler with 3 filters: \",A.shape)\n",
    "down_result = down_model(A)\n",
    "print(\"Shape of the output of a downsampler with 3 filters: \",down_result.shape)\n",
    "\n",
    "# define the generator upsampler\n",
    "def upsample(filters,size,apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0,0.02)\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(tf.keras.layers.Conv3DTranspose(filters,size,strides=2,padding='same',\n",
    "                                       kernel_initializer=initializer,use_bias=False))\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "    return result\n",
    "\n",
    "# verify the shape of the upsampler output\n",
    "up_model = upsample(3,4)\n",
    "up_result = up_model(down_result)\n",
    "print(\"Shape of the output of a up-sampler with 3 filters: \",up_result.shape)\n",
    "\n",
    "# defined the generator using the pre-defined down- and up-samplers\n",
    "def Generator(OUTPUT_CHANNELS=1):\n",
    "    inputs = tf.keras.layers.Input(shape=[DATA_HEIGHT,DATA_WIDTH,DATA_DEPTH,1])\n",
    "    down_stack=[downsample(64,4,apply_batchnorm=False), #(1,96,32,32,64)\n",
    "               downsample(128,4), #(1,48,16,16,128)\n",
    "               downsample(256,4), #(1,24,8,8,256)\n",
    "               downsample(512,4), #(1,12,4,4,512)\n",
    "               downsample(512,4), #(1,6,2,2,512)\n",
    "               downsample(512,4), #(1,3,1,1,512)\n",
    "               ]\n",
    "    up_stack=[upsample(512,4,apply_dropout=True),\n",
    "             upsample(512,4,apply_dropout=True),\n",
    "             upsample(256,4), \n",
    "             upsample(128,4),\n",
    "             upsample(64,4), \n",
    "             ]\n",
    "    initializer=tf.random_normal_initializer(0,0.02)\n",
    "    last = tf.keras.layers.Conv3DTranspose(OUTPUT_CHANNELS,4,strides=2,padding='same',\n",
    "                                          kernel_initializer=initializer,activation='linear')\n",
    "                                          \n",
    "    x = inputs\n",
    "    skips=[]\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "    skips = reversed(skips[:-1])\n",
    "    for up,skip in zip(up_stack,skips):\n",
    "        x = up(x)\n",
    "        x = tf.keras.layers.Concatenate()([x,skip])\n",
    "    x = last(x)\n",
    "    return tf.keras.Model(inputs=inputs,outputs=x)\n",
    "\n",
    "# generator model summary\n",
    "generator = Generator()\n",
    "print(\"This is the generator model summary\")\n",
    "generator.summary()\n",
    "print(\"This a the plot of the generator model\")\n",
    "\n",
    "# save a plot of the generator model\n",
    "tf.keras.utils.plot_model(generator,show_shapes=True,dpi=64)\n",
    "gen_output = generator(Asample[0],training=False)\n",
    "\n",
    "# draw an example of how the output of generator would look like\n",
    "print(\"This is an example of how the output of generator would look like:\")\n",
    "plt.imshow(tf.reduce_mean(tf.squeeze(gen_output),axis=-1),cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a521cacd",
   "metadata": {},
   "source": [
    "## Build the discriminator\n",
    "\n",
    "The discriminator is a convolutional PatchGAN classifierâ€”it tries to classify if each image _patch_ is real or not real.\n",
    "\n",
    "- Each block in the discriminator is: Convolution -> Batch normalization -> Leaky ReLU.\n",
    "- The discriminator receives 2 inputs: \n",
    "    - The input image and the target image, which it should classify as real.\n",
    "    - The input image and the generated image (the output of the generator), which it should classify as fake.\n",
    "    - Use `tf.concat([inp, tar], axis=-1)` to concatenate these 2 inputs together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84b1461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the discriminator model\n",
    "def Discriminator():\n",
    "    initializer = tf.random_normal_initializer(0.,0.02)\n",
    "    anatomy = tf.keras.layers.Input(shape=([DATA_HEIGHT,DATA_WIDTH,DATA_DEPTH,1]),\n",
    "                                   name='input_image')\n",
    "    flow = tf.keras.layers.Input(shape=([DATA_HEIGHT,DATA_WIDTH,DATA_DEPTH,1]),\n",
    "                                name='target_image')\n",
    "    x = tf.keras.layers.concatenate([anatomy,flow]) \n",
    "    down1 = downsample(64,4,False)(x)  \n",
    "    down2 = downsample(128,4)(down1) \n",
    "    down3 = downsample(256,4)(down2)  \n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding3D()(down3) \n",
    "    conv = tf.keras.layers.Conv3D(512,4,strides=1,\n",
    "                                 kernel_initializer=initializer,\n",
    "                                 use_bias=False)(zero_pad1) \n",
    "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "    zero_pad2 = tf.keras.layers.ZeroPadding3D()(leaky_relu) \n",
    "    last = tf.keras.layers.Conv3D(1,4,strides=1,\n",
    "                                 kernel_initializer=initializer)(zero_pad2)\n",
    "    return tf.keras.Model(inputs=[anatomy,flow],outputs=last)\n",
    "\n",
    "# visualize the discriminator model\n",
    "discriminator = Discriminator()\n",
    "discriminator.summary()\n",
    "tf.keras.utils.plot_model(discriminator,show_shapes=True,dpi=64)\n",
    "\n",
    "# visualize an example output of the discriminator model\n",
    "disc_out = discriminator([Asample[0],gen_output],training=False)\n",
    "plt.imshow(tf.reduce_mean(tf.squeeze(disc_out),axis=-1),cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876e9376",
   "metadata": {},
   "source": [
    "## Define the generator and discriminaotr losses\n",
    "\n",
    "cGANs learn a structured loss that penalizes a possible structure that differs from the network output and the target image.\n",
    "\n",
    "- The generator loss is a sigmoid cross-entropy loss of the generated images and an **array of ones**.\n",
    "- The pix2pix paper also mentions the L1 loss, which is a MAE (mean absolute error) between the generated image and the target image.\n",
    "- This allows the generated image to become structurally similar to the target image.\n",
    "- The formula to calculate the total generator loss is `gan_loss + LAMBDA * l1_loss`, where `LAMBDA = 100`. \n",
    "\n",
    "\n",
    "- The `discriminator_loss` function takes 2 inputs: **real images** and **generated images**.\n",
    "- `real_loss` is a sigmoid cross-entropy loss of the **real images** and an **array of ones(since these are the real images)**.\n",
    "- `generated_loss` is a sigmoid cross-entropy loss of the **generated images** and an **array of zeros (since these are the fake images)**.\n",
    "- The `total_loss` is the sum of `real_loss` and `generated_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d0dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of the generator loss\n",
    "LAMBDA = 100\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "def generator_loss(disc_generated_output,gen_ouptput,target):\n",
    "    gan_loss = loss_object(tf.ones_like(disc_generated_output),disc_generated_output)\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target-gen_output))\n",
    "    total_gan_loss = gan_loss+(LAMBDA*l1_loss)\n",
    "    return total_gan_loss,gan_loss,l1_loss\n",
    "\n",
    "# define the discriminator loss\n",
    "def discriminator_loss(disc_real_output,disc_generated_output):\n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output),disc_real_output)\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output),disc_generated_output)\n",
    "    total_disc_loss = real_loss+generated_loss\n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e7dff",
   "metadata": {},
   "source": [
    "## Define the generator and discriminator optimizers and a checkpoint-saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bd8f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizers\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4,beta_1=0.5)\n",
    "discriminator_optimizer=tf.keras.optimizers.Adam(2e-4,beta_1=0.5)\n",
    "\n",
    "output_dir = './pix2pix_output' # the directory to save the model outputs\n",
    "if not tf.io.gfile.isdir(output_dir):\n",
    "    tf.io.gfile.mkdir(output_dir) # if the output directory does not exist, creates it\n",
    "    \n",
    "# define the checkpoint directory\n",
    "checkpoint_dir = tf.io.gfile.join(output_dir,'training_checkpoints') # where to save model ch\n",
    "checkpoint_prefix = tf.io.gfile.join(checkpoint_dir,\"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                discrimnator_optimizer=discriminator_optimizer,\n",
    "                                generator=generator,\n",
    "                                discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e02b85",
   "metadata": {},
   "source": [
    "## Generate images\n",
    "\n",
    "Write a function to plot some images during training.\n",
    "\n",
    "- Pass images from the test set to the generator.\n",
    "- The generator will then translate the input image into the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88661e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate images over the test dataset\n",
    "def generate_images(model,test_input,tar):\n",
    "    prediction = model(test_input,training=True)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    display_list = [test_input[0],tar[0],prediction[0]]\n",
    "    title=['input_image','target_image','predicted image']\n",
    "    for i in range(3):\n",
    "        plt.subplot(1,3,i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.reduce_max(tf.squeeze(display_list[i]),axis=-1),cmap='jet',clim=[0,2])\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "# visualize example images\n",
    "generate_images(generator,Asample[0],Bsample[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e09a9a",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- For each example input generates an output.\n",
    "- The discriminator receives the `input_image` and the generated image as the first input. The second input is the `input_image` and the `target_image`.\n",
    "- Next, calculate the generator and the discriminator loss.\n",
    "- Then, calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a603c469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 192, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def train_step(anatomy,flow,step):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = generator(anatomy,training=True)\n",
    "        disc_real_output = discriminator([anatomy,flow],training=True)\n",
    "        disc_generated_output = discriminator([anatomy,gen_output],training=True)\n",
    "        gen_total_loss,gen_gan_loss,gen_l1_loss = generator_loss(disc_generated_output,\n",
    "                                                                gen_output,\n",
    "                                                                flow)\n",
    "        disc_loss = discriminator_loss(disc_real_output,disc_generated_output)\n",
    "    generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                           generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                                discriminator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients,generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,discriminator.trainable_variables))\n",
    "\n",
    "epochs = 100 # specifiy number of epochs to train the network\n",
    "\n",
    "sample_dir = tf.io.gfile.join(output_dir,'samples_training') # directory to save example .mat files\n",
    "if not tf.io.gfile.isdir(sample_dir):\n",
    "    tf.io.gfile.mkdir(sample_dir)\n",
    "    \n",
    "# Restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint_address = tf.train.latest_checkpoint(checkpoint_dir) # returns the address of last checkpoint\n",
    "epochs_so_far = 0 \n",
    "if checkpoint_address: # verifies if the checkpoint exists\n",
    "    checkpoint.restore(checkpoint_address) # restores to the last checkpoint\n",
    "    hyphen = checkpoint_address.index('-') # finds the location of hyphen in the checkpoint address\n",
    "    epochs_so_far = int(checkpoint_address[hyphen+1:]) # finds the number trained epochs\n",
    "    print(\"Restored the model from epoch {}\".format(epochs_so_far))\n",
    "\n",
    "for epoch in tqdm.trange(epochs_so_far+1,epochs+epochs_so_far+1,desc=\"Outer Epoch\",total=epochs):\n",
    "    for anatomy,flow in tqdm.tqdm(dataset_train,desc=\"Inner Epoch\",total=DATA_SIZE):\n",
    "        train_step(anatomy,flow)\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix) # save a checkpoint after each epoch\n",
    "    example_anatomy,example_flow = next(dataset_test) # load an example test dataset after each epoch\n",
    "    generate_images(generator,example_anatomy,example_flow) # generate example images after each epoch\n",
    "    for ii in range(len(Asample)):\n",
    "        A = tf.reshape(Asample[ii],(1,DATA_HEIGHT,DATA_WIDTH,DATA_DEPTH,1))\n",
    "        B = tf.reshape(Bsample[ii],(1,DATA_HEIGHT,DATA_WIDTH,DATA_DEPTH,1))\n",
    "        A2B = generator(A,training=True)\n",
    "        anatomy = tf.squeeze(A.numpy())\n",
    "        flow = tf.squeeze(B.numpy())\n",
    "        anatomy2flow = tf.squeeze(A2B.numpy())  \n",
    "        filename1 = tf.io.gfile.join(sample_dir,'iter-%03u-%02u.mat' % (epoch,ii))\n",
    "        scipy.io.savemat(filename1,{'anatomy':anatomy.numpy(),\n",
    "                        'flow':flow.numpy(),\n",
    "                        'anatomy2flow':anatomy2flow.numpy()})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
