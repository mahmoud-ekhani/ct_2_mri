{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1538178a",
   "metadata": {},
   "source": [
    "## Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c35613a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import functools\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tqdm\n",
    "import scipy\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "# specify the gpu to be utilized\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a542fcd",
   "metadata": {},
   "source": [
    "## define the data and model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dd4b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image size\n",
    "height_crop_size = 160 \n",
    "width_crop_size = 64\n",
    "slice_size = 64\n",
    "\n",
    "# model training parameters\n",
    "batch_size = 1\n",
    "epochs = 100\n",
    "epoch_decay = epochs // 10 # number of epoch to start decaying the learning rate\n",
    "\n",
    "# cycleGAN parameters\n",
    "lr = 0.0002 # learning rate\n",
    "beta_1 = 0.5 # weight\n",
    "adversarial_loss_mode = 'lsgan' # choices = ['gan', 'hinge_v1', 'hinge_v2', 'lsgan', 'wgan']\n",
    "gradient_penalty_mode = 'none' # choices = ['none', 'dragan', 'wgan-gp']\n",
    "gradient_penalty_weight = 10.0\n",
    "cycle_loss_weight = 10.0\n",
    "identity_loss_weight = 2.0\n",
    "pool_size = 50 # pool size of fake samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce7fcde",
   "metadata": {},
   "source": [
    "## Load the training data\n",
    "\n",
    "The following code defines a TensorFlow program that loads and processes a training dataset in the TensorFlow Record format.\n",
    "\n",
    "1. The ``center_crop`` function is used to crop an image around the center to a specified size. The function takes an image tensor and the desired size as input, and returns the cropped image tensor.\n",
    "\n",
    "2. The ``parser`` function is used to parse a single TensorFlow Record from the dataset. The function takes a single TensorFlow Record as input and returns the decoded anatomy and flow tensors.\n",
    "\n",
    "3. The ``fldr`` variable defines the folder where the TensorFlow Record files are located. The tfrecord_paths variable is a list of the TensorFlow Record file paths in the folder.\n",
    "\n",
    "4. The ``dataset_train`` variable is a TensorFlow Dataset that contains the TensorFlow Record files from the fldr folder. The dataset_train is processed using interleave and map functions to efficiently load and parse the TensorFlow Record files.\n",
    "\n",
    "5. The ``data_size`` variable is the total size of the training dataset. The dataset is then ``shuffled`` and ``batched`` with a specified batch size. The ``len_dataset`` variable is the number of batches in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d43c7f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is composed of ['tfrecords\\\\bav_train.tfrecords', 'tfrecords\\\\ct2mri_train.tfrecords', 'tfrecords\\\\tav_train.tfrecords']\n",
      "Size of the loaded training data set: 1095\n"
     ]
    }
   ],
   "source": [
    "# crop the enlarged images back to original size around the center\n",
    "# This line of code is defining the center_crop function which takes two inputs, an image and a size. \n",
    "@tf.function\n",
    "def center_crop(image, size):\n",
    "    # This line of code checks if the size input is a tuple or a list. \n",
    "    # If it's not, the size is set to a list containing the size value twice. \n",
    "    if not isinstance(size, (tuple, list)):\n",
    "        size = [size, size]\n",
    "    # This line of code calculates the offset height value. \n",
    "    # The offset height is the difference between the height of the image and the desired height size, divided by 2.\n",
    "    offset_height = (tf.shape(image)[-3]-size[0])//2\n",
    "    # This line of code calculates the offset width value. \n",
    "    # The offset width is the difference between the width of the image and the desired width size, divided by 2.\n",
    "    offset_width = (tf.shape(image)[-2]-size[1])//2\n",
    "    # This line of code crops the image based on the calculated offset values and desired size values. \n",
    "    return tf.image.crop_to_bounding_box(image,offset_height,offset_width,size[0],size[1])\n",
    "\n",
    "\n",
    "# Parse a single example from a tfrecord file\n",
    "def parser(tfrecord):\n",
    "    # Parse features from the tfrecord file\n",
    "    feature = tf.io.parse_single_example(tfrecord,\n",
    "                                          {'anatomy': tf.io.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "                                           'flow'  : tf.io.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "                                           'height': tf.io.FixedLenFeature(shape=[], dtype=tf.int64),\n",
    "                                           'width' : tf.io.FixedLenFeature(shape=[], dtype=tf.int64),\n",
    "                                           'depth' : tf.io.FixedLenFeature(shape=[], dtype=tf.int64)})\n",
    "    # Convert height, width and depth from int64 to int32\n",
    "    height = tf.cast(feature[\"height\"], tf.int32)\n",
    "    width  = tf.cast(feature[\"width\"], tf.int32)\n",
    "    depth  = tf.cast(feature[\"depth\"], tf.int32)\n",
    "    \n",
    "    # Decode the anatomy feature from raw bytes to float32\n",
    "    anatomy = tf.io.decode_raw(feature['anatomy'], tf.float32) \n",
    "    # Reshape the anatomy data to [height, width, depth]\n",
    "    anatomy = tf.reshape(anatomy, [height, width, depth])\n",
    "    # Crop the anatomy image\n",
    "    anatomy = center_crop(anatomy, [height_crop_size,width_crop_size])\n",
    "    \n",
    "    # Decode the flow feature from raw bytes to float32\n",
    "    flow = tf.io.decode_raw(feature['flow'], tf.float32) \n",
    "    # Reshape the flow data to [height, width, depth]\n",
    "    flow = tf.reshape(flow, [height, width, depth])\n",
    "    # Crop the flow image\n",
    "    flow = center_crop(flow, [height_crop_size,width_crop_size])\n",
    "    \n",
    "    # Return the anatomy and flow images as a tuple\n",
    "    return anatomy, flow\n",
    "\n",
    "\n",
    "# set the directory to store the training data\n",
    "fldr = 'tfrecords'\n",
    "\n",
    "# get the path of all training data files in the directory\n",
    "tfrecord_paths = tf.io.gfile.glob(fldr+\"/*train*\")\n",
    "\n",
    "# print the number of files in the training dataset\n",
    "print(f\"The training dataset is composed of {len(tfrecord_paths)} files:\\n{tfrecord_paths}\")\n",
    "\n",
    "# create a dataset from the training data file paths\n",
    "dataset_train = tf.data.Dataset.list_files(tfrecord_paths)\n",
    "\n",
    "# interleave multiple training data files for parallel reading\n",
    "dataset_train = dataset_train.interleave(lambda filename: tf.data.TFRecordDataset(filename),\n",
    "                                         cycle_length=len(tfrecord_paths),\n",
    "                                         num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# apply the parsing function to each data sample\n",
    "dataset_train = dataset_train.map(map_func=parser, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# get the number of samples in the dataset\n",
    "data_size = sum(1 for _ in dataset_train)\n",
    "\n",
    "# shuffle the samples\n",
    "dataset_train = dataset_train.shuffle(buffer_size=data_size)\n",
    "\n",
    "# group the samples into batches\n",
    "dataset_train = dataset_train.batch(batch_size)\n",
    "\n",
    "# calculate the number of batches\n",
    "len_dataset = int(data_size/batch_size)\n",
    "\n",
    "# print the size of the loaded training dataset\n",
    "print(f\"Size of the loaded training data set: {len_dataset} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb2e28",
   "metadata": {},
   "source": [
    "## Generator and discriminator models\n",
    "\n",
    "1. **Generator**\n",
    "- **generator** is the 3D generator model for a convolutional neural network (CNN) using the functions encoder_layer and decoder_layer. The model has an input layer with a specified shape and applies multiple encoder and decoder layers to generate an output.\n",
    "    - The function ``encoder_layer`` implements an encoder layer for a 3D Convolutional Neural Network (3D-CNN) model. It takes as input a feature map \"x_con\" and applies multiple 3D Convolution, Batch Normalization, and Leaky ReLU activation operations to produce a new feature map. The new feature map is concatenated with the input feature map, and this concatenation is fed as input to the next iteration of 3D Convolution, Batch Normalization, and Leaky ReLU. A pooling operation is optionally applied at the end of the encoding layer, which is specified by the ``pool`` argument. The pooling operation is an Average Pooling operation that reduces the spatial dimensions of the feature map by a factor of 2.\n",
    "\n",
    "    - The ``decoder layer`` defines a generator decoder layer. The layer takes three inputs, input_, x, and ch, which represent the tensor to be upsampled, a tensor to concatenate with the upsampled tensor, and the number of channels, respectively. The layer performs a transposed convolution with filters equal to 20, kernel_size equal to [2,2,1], and strides equal to [2,2,1] to upsample input_. The upsampled tensor and x are then concatenated along the last axis.\n",
    "\n",
    "2. **Discriminator**\n",
    "- The ``downsample`` creates a 3D convolutional neural network (CNN) layer followed by batch normalization and leaky rectified linear unit (ReLU) activation is is used in the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11c0b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(x_con, iterations, name, training, pool=True, filters=20, kernel_size=(3, 3, 3)):\n",
    "    # assign x_con to x\n",
    "    x = x_con\n",
    "    # create batch normalization layer\n",
    "    bn = tf.keras.layers.BatchNormalization()\n",
    "    # create leaky relu layer\n",
    "    relu = tf.keras.layers.LeakyReLU()\n",
    "    # loop through the number of iterations\n",
    "    for i in range(iterations):\n",
    "        # apply 3D convolution layer with specified filters and kernel size, with padding \"SAME\"\n",
    "        x = tf.keras.layers.Conv3D(filters, \n",
    "                                   kernel_size, \n",
    "                                   padding='SAME',\n",
    "                                   name=f\"conv_{i}\")(x)\n",
    "        # apply batch normalization to the output\n",
    "        x = bn(x, training=training)\n",
    "        # apply leaky relu activation to the output\n",
    "        x = relu(x)\n",
    "        # concatenate x and x_con along the last axis\n",
    "        x_con = tf.concat([x, x_con], axis=-1)\n",
    "    # if pool is True\n",
    "    if pool:\n",
    "        # apply average pooling with specified pool size and strides, and data format 'channels_last'\n",
    "        pool = tf.keras.layers.AveragePooling3D(pool_size=(2, 2, 1), \n",
    "                                                strides=(2, 2, 1),\n",
    "                                                data_format='channels_last')(x_con)\n",
    "        # return both x_con and pool\n",
    "        return x_con, pool\n",
    "    # if pool is False\n",
    "    return x_con\n",
    "    # return only x_con\n",
    "\n",
    "\n",
    "    \n",
    "def decoder_layer(inputs, x, channels, name, upscale=(2, 2, 2), filters=20, kernel_size=(2, 2, 1)):\n",
    "    with tf.name_scope(f\"decoder_block_{name}\"):\n",
    "        # Upsample the input with a transposed convolution\n",
    "        up = tf.keras.layers.Conv3DTranspose(filters=filters,\n",
    "                                             kernel_size=kernel_size,\n",
    "                                             strides=upscale,\n",
    "                                             padding='SAME',\n",
    "                                             name=f\"upsample_{name}\",\n",
    "                                             use_bias=False)(inputs)\n",
    "        # Concatenate the upsampled input with the other input `x`\n",
    "        up = tf.concat([up, x], axis=-1, name=f\"merge_{name}\")\n",
    "    return up\n",
    "\n",
    "def generator():\n",
    "    \"\"\"\n",
    "    Function to define the generator model of the CNN\n",
    "    \n",
    "    Returns:\n",
    "    model (tf.keras.Model): The generator model\n",
    "    \"\"\"\n",
    "    # Input layer with specified shape\n",
    "    input_ = tf.keras.layers.Input(shape=[height_crop_size,width_crop_size,slice_size,1])\n",
    "    \n",
    "    # Encoder layers with specified iterations and name\n",
    "    conv1, pool1 = encoder_layer(input_, iterations=2, name=\"encode_im1\", training=True, pool=True)\n",
    "    conv2, pool2 = encoder_layer(pool1, iterations=4, name=\"encode_im2\", training=True, pool=True)\n",
    "    conv3, pool3 = encoder_layer(pool2, iterations=6, name=\"encode_im3\", training=True, pool=True)\n",
    "    conv4 = encoder_layer(pool3, iterations=8, name=\"encode_im4\", training=True, pool=False)\n",
    "    \n",
    "    # Decoder layers with specified name\n",
    "    up1 = decoder_layer(conv4, conv3, 10, name=12)\n",
    "    conv7 = encoder_layer(up1, iterations=6, name=\"conv_im6\", training=True, pool=False)\n",
    "    up2 = decoder_layer(conv7, conv2, 8, name=21)\n",
    "    conv8 = encoder_layer(up2, iterations=4, name=\"encode_im7\", training=True, pool=False)\n",
    "    up3 = decoder_layer(conv8, conv1, 6, name=32)\n",
    "    conv9 = encoder_layer(up3, iterations=2, name=\"encode_im8\", training=True, pool=False)\n",
    "    \n",
    "    # Final Conv3D layer\n",
    "    conv10 = tf.keras.layers.Conv3D(1, (1,1,1), name='logits_re_im', padding='SAME')(conv9)\n",
    "    \n",
    "    # Model definition\n",
    "    model = tf.keras.Model(inputs=input_, outputs=conv10)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# disciminator downsampler\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    \"\"\"\n",
    "    Function to create a downsampling layer in the generator network\n",
    "    \n",
    "    Parameters:\n",
    "    filters (int): Number of filters in the Conv3D layer\n",
    "    size (int): The size of the Conv3D layer\n",
    "    apply_batchnorm (bool, optional): If True, adds BatchNormalization layer after Conv3D. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "    result (tf.keras.Sequential): The downsampling layer\n",
    "    \"\"\"\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    result = tf.keras.Sequential()\n",
    "    \n",
    "    # Conv3D layer with specified number of filters and kernel size\n",
    "    result.add(tf.keras.layers.Conv3D(filters, kernel_size=[3,3,3], padding='same',\n",
    "                                      kernel_initializer=initializer))\n",
    "    \n",
    "    # Optional BatchNormalization layer and leaky ReLU activation\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "        result.add(tf.keras.layers.LeakyReLU())\n",
    "    \n",
    "    return result\n",
    "\n",
    "def discriminator():\n",
    "    # Initialize a random normal initializer with mean 0 and standard deviation 0.02\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    \n",
    "    # Create an input layer for the input image with shape [height_crop_size, width_crop_size, slice_size, 1]\n",
    "    inp = tf.keras.layers.Input(shape=[height_crop_size, width_crop_size, slice_size, 1],\n",
    "                                name='input_image')\n",
    "    \n",
    "    # Create an input layer for the target image with shape [height_crop_size, width_crop_size, slice_size, 1]\n",
    "    tar = tf.keras.layers.Input(shape=[height_crop_size, width_crop_size, slice_size, 1],\n",
    "                                name='target_image')\n",
    "    \n",
    "    # Concatenate the input and target images\n",
    "    x = tf.keras.layers.concatenate([inp, tar])\n",
    "    \n",
    "    # Apply the downsample function with 16 filters and a kernel size of 2, and no batch normalization\n",
    "    down1 = downsample(16, 2, False)(x)\n",
    "    \n",
    "    # Apply the downsample function with 32 filters and a kernel size of 2, and batch normalization\n",
    "    down2 = downsample(32, 2)(down1)\n",
    "    \n",
    "    # Apply the downsample function with 64 filters and a kernel size of 2, and batch normalization\n",
    "    down3 = downsample(64, 2)(down2)\n",
    "    \n",
    "    # Apply a zero padding layer to down3\n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding3D()(down3)\n",
    "    \n",
    "    # Apply a Conv3D layer with 128 filters and a kernel size of 4, and stride of 1\n",
    "    conv = tf.keras.layers.Conv3D(128, 4, strides=1, padding='SAME')(zero_pad1)\n",
    "    \n",
    "    # Apply a batch normalization layer to the output of the Conv3D layer\n",
    "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "    \n",
    "    # Apply a leaky ReLU activation layer to the output of the batch normalization layer\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "    \n",
    "    # Apply a zero padding layer to the output of the leaky ReLU activation layer\n",
    "    zero_pad2 = tf.keras.layers.ZeroPadding3D()(leaky_relu)\n",
    "    \n",
    "    # Apply a Conv3D layer with 1 filter and a kernel size of 4, and stride of 1\n",
    "    last = tf.keras.layers.Conv3D(1, 4, strides=1, padding='SAME')(zero_pad2)\n",
    "    \n",
    "    # Return a Model with input layers [inp, tar] and output layer `last`\n",
    "    return tf.keras.Model(inputs=[inp, tar], outputs=last)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9714f169",
   "metadata": {},
   "source": [
    "## Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50993a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(disc, gen, anatomy, flow, gen_optimizer, disc_optimizer):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:  \n",
    "        # Normalize anatomy data between 0 and 1\n",
    "        anatomy = tf.keras.utils.normalize(anatomy)\n",
    "\n",
    "        # Generate anatomy2flow\n",
    "        anatomy2flow = gen(anatomy) \n",
    "\n",
    "        # Calculate magnitude of anatomy2flow\n",
    "        mag = tf.abs(tf.squeeze(anatomy2flow))\n",
    "\n",
    "        # Loss functions\n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "        loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "        # Evaluate discriminator on real data and generated data\n",
    "        discriminator_real_output = disc([anatomy, flow], training=True)\n",
    "        discriminator_generated_output = disc([anatomy, anatomy2flow], training=True)\n",
    "\n",
    "        # Calculate real loss and generated loss for discriminator\n",
    "        real_loss = loss_object(tf.ones_like(discriminator_real_output), discriminator_real_output)\n",
    "        generated_loss = loss_object(tf.zeros_like(discriminator_generated_output), discriminator_generated_output)\n",
    "        total_discriminator_loss = real_loss + generated_loss\n",
    "\n",
    "        # Calculate GAN loss\n",
    "        gan_loss = loss_object(tf.ones_like(discriminator_generated_output), discriminator_generated_output)\n",
    "\n",
    "        # Calculate gradients and laplacian for both magnitude and flow\n",
    "        flow = tf.squeeze(flow)\n",
    "        ai_grady = np.gradient(mag[...].numpy())\n",
    "        man_grady = np.gradient(flow[...].numpy())\n",
    "        ai_grady2 = scipy.ndimage.laplace(mag.numpy())\n",
    "        man_grady2 = scipy.ndimage.laplace(flow.numpy())\n",
    "\n",
    "        # Calculate structural similarity (SSIM)\n",
    "        loss_ssimy = tf.reduce_mean(tf.image.ssim(mag[...]/tf.reduce_max(mag[...]), \\\n",
    "                                                 flow[...]/tf.reduce_max(flow[...]),1.0))\n",
    "\n",
    "        # Calculate final loss function\n",
    "        loss_fn = gan_loss + 1000 * mse(mag, flow) + (1-loss_ssimy) + 1000 * mse(ai_grady, man_grady) + 10 * mse(ai_grady2, man_grady2)\n",
    "    \n",
    "    # Trainable variables for discriminator and generator\n",
    "    variables1 = disc.trainable_variables     \n",
    "    variables = gen.trainable_variables \n",
    "    \n",
    "    # Calculate gradients for generator and discriminator\n",
    "    gradients = gen_tape.gradient(loss_fn, variables)\n",
    "    gradients1 = disc_tape.gradient(total_discriminator_loss, variables1)\n",
    "    \n",
    "    # Apply gradients using optimizers\n",
    "    gen_optimizer.apply_gradients(zip(gradients,variables)) \n",
    "    disc_optimizer.apply_gradients(zip(gradients1,variables1))\n",
    "    return loss_fn, loss_ssimy , gan_loss\n",
    "\n",
    "# define LinearDecay schedular\n",
    "class LinearDecay(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_learning_rate, total_steps, step_decay):\n",
    "        super(LinearDecay, self).__init__()\n",
    "        self._initial_learning_rate = initial_learning_rate\n",
    "        self._steps = total_steps\n",
    "        self._step_decay = step_decay\n",
    "        self.current_learning_rate = tf.Variable(initial_value=initial_learning_rate,\\\n",
    "                                                 trainable=False, dtype=tf.float32)\n",
    "    def __call__(self, step):\n",
    "        self.current_learning_rate.assign(tf.cond(\n",
    "            step >= self._step_decay,\n",
    "            true_fn=lambda: self._initial_learning_rate * (1 - 1 /  (self._steps - self._step_decay) \\\n",
    "                                                           *(step - self._step_decay)),\n",
    "            false_fn=lambda: self._initial_learning_rate\n",
    "        ))\n",
    "        return self.current_learning_rate\n",
    "\n",
    "# define the generator and discriminator optimizers\n",
    "llr = LinearDecay(0.0002, epochs*len_dataset, epoch_decay*len_dataset)\n",
    "gen_optimizer = tf.keras.optimizers.Adam(learning_rate=llr) \n",
    "disc_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e45301",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "This code is checking if the output directory and checkpoint directory exists, if not it creates them. Then it defines the generator and discriminator models, the checkpoint prefix, and checkpoint object. Finally, it checks if a checkpoint exists, and if so, it restores the model from the latest checkpoint by using the ``tf.train.latest_checkpoint`` method. The number of trained epochs is also found by parsing the checkpoint address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d694880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output directory\n",
    "output_dir = './haben_output'\n",
    "\n",
    "# If the output directory does not exist, create it\n",
    "if not tf.io.gfile.isdir(output_dir):\n",
    "    tf.io.gfile.mkdir(output_dir)\n",
    "\n",
    "# Define the generator and discriminator models\n",
    "gen = generator()\n",
    "disc = discriminator()\n",
    "\n",
    "# Define the checkpoint directory\n",
    "checkpoint_dir = tf.io.gfile.join(output_dir, 'training_checkpoints')\n",
    "np\n",
    "# If the checkpoint directory does not exist, create it\n",
    "if not tf.io.gfile.isdir(checkpoint_dir):\n",
    "    tf.io.gfile.mkdir(checkpoint_dir)\n",
    "\n",
    "# Define the checkpoint prefix and the checkpoint object\n",
    "checkpoint_prefix = tf.io.gfile.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(gen_optimizer=gen_optimizer,\n",
    "                                 disc_optimizer=disc_optimizer,\n",
    "                                 gen=gen,\n",
    "                                 disc=disc)\n",
    "\n",
    "# Get the address of the latest checkpoint\n",
    "checkpoint_address = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "# If a checkpoint exists, restore the model from it\n",
    "epochs_so_far = 0\n",
    "if checkpoint_address:\n",
    "    checkpoint.restore(checkpoint_address)\n",
    "    hyphen = checkpoint_address.index('-')\n",
    "    epochs_so_far = int(checkpoint_address[hyphen+1:])\n",
    "    print(\"Restored the model from epoch {}\".format(epochs_so_far))\n",
    "\n",
    "\n",
    "# Train the model for each outer epoch\n",
    "for epoch in tqdm.trange(epochs_so_far+1,epochs+1,desc=\"Outer Epoch\",total=epochs-epochs_so_far):\n",
    "    # Train the model for each inner epoch\n",
    "    for anatomy,flow in tqdm.tqdm(dataset_train,desc=\"Inner Epoch\",total=len_dataset):\n",
    "        # Reshape the anatomy and flow data to a specific shape\n",
    "        anatomy = tf.reshape(anatomy,(1,height_crop_size,width_crop_size,slice_size,1))\n",
    "        flow = tf.reshape(flow,(1,height_crop_size,width_crop_size,slice_size,1))\n",
    "        # Train the generator and discriminator models\n",
    "        loss, sss, gn = train_step(disc,gen,anatomy,flow, gen_optimizer, disc_optimizer)\n",
    "    # Save the model checkpoint after each outer epoch\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
