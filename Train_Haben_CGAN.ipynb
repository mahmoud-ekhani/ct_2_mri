{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1538178a",
   "metadata": {},
   "source": [
    "## Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35613a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 10:58:08.781109: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-07 10:58:08.865186: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import numpy as np\n",
    "import tqdm\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a542fcd",
   "metadata": {},
   "source": [
    "## define the data and model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dd4b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image size\n",
    "height_crop_size = 160 \n",
    "width_crop_size = 64\n",
    "slice_size = 64\n",
    "\n",
    "# model training parameters\n",
    "batch_size = 1\n",
    "epochs = 40\n",
    "epoch_decay = epochs // 10 # number of epoch to start decaying the learning rate\n",
    "\n",
    "# cycleGAN parameters\n",
    "lr = 0.0002 # learning rate\n",
    "beta_1 = 0.5 # weight\n",
    "adversarial_loss_mode = 'lsgan' # choices = ['gan', 'hinge_v1', 'hinge_v2', 'lsgan', 'wgan']\n",
    "gradient_penalty_mode = 'none' # choices = ['none', 'dragan', 'wgan-gp']\n",
    "gradient_penalty_weight = 10.0\n",
    "cycle_loss_weight = 10.0\n",
    "identity_loss_weight = 2.0\n",
    "pool_size = 50 # pool size of fake samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce7fcde",
   "metadata": {},
   "source": [
    "## Load the training data\n",
    "\n",
    "The following code defines a TensorFlow program that loads and processes a training dataset in the TensorFlow Record format.\n",
    "\n",
    "1. The ``center_crop`` function is used to crop an image around the center to a specified size. The function takes an image tensor and the desired size as input, and returns the cropped image tensor.\n",
    "\n",
    "2. The ``parser`` function is used to parse a single TensorFlow Record from the dataset. The function takes a single TensorFlow Record as input and returns the decoded anatomy and flow tensors.\n",
    "\n",
    "3. The ``fldr`` variable defines the folder where the TensorFlow Record files are located. The tfrecord_paths variable is a list of the TensorFlow Record file paths in the folder.\n",
    "\n",
    "4. The ``dataset_train`` variable is a TensorFlow Dataset that contains the TensorFlow Record files from the fldr folder. The dataset_train is processed using interleave and map functions to efficiently load and parse the TensorFlow Record files.\n",
    "\n",
    "5. The ``data_size`` variable is the total size of the training dataset. The dataset is then ``shuffled`` and ``batched`` with a specified batch size. The ``len_dataset`` variable is the number of batches in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d43c7f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is composed of 3 files:\n",
      "['training_data/tav_train.tfrecords', 'training_data/bav_train.tfrecords', 'training_data/ct2mri_train.tfrecords']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 10:58:18.194946: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-07 10:58:19.121215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38248 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:2f:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mem1342/.conda/envs/tensorflow-2.6-py38/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "Size of the loaded training data set: 1095 batches\n"
     ]
    }
   ],
   "source": [
    "# crop the enlarged images back to original size around the center\n",
    "# This line of code is defining the center_crop function which takes two inputs, an image and a size. \n",
    "@tf.function\n",
    "def center_crop(image, size):\n",
    "    # This line of code checks if the size input is a tuple or a list. \n",
    "    # If it's not, the size is set to a list containing the size value twice. \n",
    "    if not isinstance(size, (tuple, list)):\n",
    "        size = [size, size]\n",
    "    # This line of code calculates the offset height value. \n",
    "    # The offset height is the difference between the height of the image and the desired height size, divided by 2.\n",
    "    offset_height = (tf.shape(image)[-3]-size[0])//2\n",
    "    # This line of code calculates the offset width value. \n",
    "    # The offset width is the difference between the width of the image and the desired width size, divided by 2.\n",
    "    offset_width = (tf.shape(image)[-2]-size[1])//2\n",
    "    # This line of code crops the image based on the calculated offset values and desired size values. \n",
    "    return tf.image.crop_to_bounding_box(image,offset_height,offset_width,size[0],size[1])\n",
    "\n",
    "\n",
    "# Parse a single example from a tfrecord file\n",
    "def parser(tfrecord):\n",
    "    # Parse features from the tfrecord file\n",
    "    feature = tf.io.parse_single_example(tfrecord,\n",
    "                                          {'anatomy': tf.io.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "                                           'flow'  : tf.io.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "                                           'height': tf.io.FixedLenFeature(shape=[], dtype=tf.int64),\n",
    "                                           'width' : tf.io.FixedLenFeature(shape=[], dtype=tf.int64),\n",
    "                                           'depth' : tf.io.FixedLenFeature(shape=[], dtype=tf.int64)})\n",
    "    # Convert height, width and depth from int64 to int32\n",
    "    height = tf.cast(feature[\"height\"], tf.int32)\n",
    "    width  = tf.cast(feature[\"width\"], tf.int32)\n",
    "    depth  = tf.cast(feature[\"depth\"], tf.int32)\n",
    "    \n",
    "    # Decode the anatomy feature from raw bytes to float32\n",
    "    anatomy = tf.io.decode_raw(feature['anatomy'], tf.float32) \n",
    "    # Reshape the anatomy data to [height, width, depth]\n",
    "    anatomy = tf.reshape(anatomy, [height, width, depth])\n",
    "    # Crop the anatomy image\n",
    "    anatomy = center_crop(anatomy, [height_crop_size,width_crop_size])\n",
    "    \n",
    "    # Decode the flow feature from raw bytes to float32\n",
    "    flow = tf.io.decode_raw(feature['flow'], tf.float32) \n",
    "    # Reshape the flow data to [height, width, depth]\n",
    "    flow = tf.reshape(flow, [height, width, depth])\n",
    "    # Crop the flow image\n",
    "    flow = center_crop(flow, [height_crop_size,width_crop_size])\n",
    "    \n",
    "    # Return the anatomy and flow images as a tuple\n",
    "    return anatomy, flow\n",
    "\n",
    "\n",
    "# set the directory to store the training data\n",
    "fldr = 'training_data'\n",
    "\n",
    "# get the path of all training data files in the directory\n",
    "tfrecord_paths = tf.io.gfile.glob(fldr+\"/*train*\")\n",
    "\n",
    "# print the number of files in the training dataset\n",
    "print(f\"The training dataset is composed of {len(tfrecord_paths)} files:\\n{tfrecord_paths}\")\n",
    "\n",
    "# create a dataset from the training data file paths\n",
    "dataset_train = tf.data.Dataset.list_files(tfrecord_paths)\n",
    "\n",
    "# interleave multiple training data files for parallel reading\n",
    "dataset_train = dataset_train.interleave(lambda filename: tf.data.TFRecordDataset(filename),\n",
    "                                         cycle_length=len(tfrecord_paths),\n",
    "                                         num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# apply the parsing function to each data sample\n",
    "dataset_train = dataset_train.map(map_func=parser, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# get the number of samples in the dataset\n",
    "data_size = sum(1 for _ in dataset_train)\n",
    "\n",
    "# shuffle the samples\n",
    "dataset_train = dataset_train.shuffle(buffer_size=data_size)\n",
    "\n",
    "# group the samples into batches\n",
    "dataset_train = dataset_train.batch(batch_size)\n",
    "\n",
    "# calculate the number of batches\n",
    "len_dataset = int(data_size/batch_size)\n",
    "\n",
    "# print the size of the loaded training dataset\n",
    "print(f\"Size of the loaded training data set: {len_dataset} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb2e28",
   "metadata": {},
   "source": [
    "## Generator and discriminator models\n",
    "\n",
    "1. **Generator**\n",
    "- **generator** is the 3D generator model for a convolutional neural network (CNN) using the functions encoder_layer and decoder_layer. The model has an input layer with a specified shape and applies multiple encoder and decoder layers to generate an output.\n",
    "    - The function ``encoder_layer`` implements an encoder layer for a 3D Convolutional Neural Network (3D-CNN) model. It takes as input a feature map \"x_con\" and applies multiple 3D Convolution, Batch Normalization, and Leaky ReLU activation operations to produce a new feature map. The new feature map is concatenated with the input feature map, and this concatenation is fed as input to the next iteration of 3D Convolution, Batch Normalization, and Leaky ReLU. A pooling operation is optionally applied at the end of the encoding layer, which is specified by the ``pool`` argument. The pooling operation is an Average Pooling operation that reduces the spatial dimensions of the feature map by a factor of 2.\n",
    "\n",
    "    - The ``decoder layer`` defines a generator decoder layer. The layer takes three inputs, input_, x, and ch, which represent the tensor to be upsampled, a tensor to concatenate with the upsampled tensor, and the number of channels, respectively. The layer performs a transposed convolution with filters equal to 20, kernel_size equal to [2,2,1], and strides equal to [2,2,1] to upsample input_. The upsampled tensor and x are then concatenated along the last axis.\n",
    "\n",
    "2. **Discriminator**\n",
    "- The ``downsample`` creates a 3D convolutional neural network (CNN) layer followed by batch normalization and leaky rectified linear unit (ReLU) activation is is used in the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11c0b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(x_con, iterations, name, training, pool=True, filters=20, kernel_size=(3, 3, 3)):\n",
    "    with tf.name_scope(\"encoder_block_{}\".format(name)):\n",
    "        # assign x_con to x\n",
    "        x = x_con\n",
    "        # create batch normalization layer\n",
    "        bn = tf.keras.layers.BatchNormalization()\n",
    "        # create leaky relu layer\n",
    "        relu = tf.keras.layers.LeakyReLU()\n",
    "        # loop through the number of iterations\n",
    "        for i in range(iterations):\n",
    "            # apply 3D convolution layer with specified filters and kernel size, with padding \"SAME\"\n",
    "            x = tf.keras.layers.Conv3D(filters, \n",
    "                                       kernel_size, \n",
    "                                       padding='SAME')(x)\n",
    "            # apply batch normalization to the output\n",
    "            x = bn(x, training=training)\n",
    "            # apply leaky relu activation to the output\n",
    "            x = relu(x)\n",
    "            # concatenate x and x_con along the last axis\n",
    "            x_con = tf.concat([x, x_con], axis=-1)\n",
    "        # if pool is True\n",
    "        if pool:\n",
    "            # apply average pooling with specified pool size and strides, and data format 'channels_last'\n",
    "            pool = tf.keras.layers.AveragePooling3D(pool_size=(2, 2, 1), \n",
    "                                                    strides=(2, 2, 1),\n",
    "                                                    data_format='channels_last')(x_con)\n",
    "            # return both x_con and pool\n",
    "            return x_con, pool\n",
    "        # if pool is False\n",
    "        return x_con\n",
    "        # return only x_con\n",
    "\n",
    "\n",
    "    \n",
    "def decoder_layer(inputs, x, channels, name, upscale=(2, 2, 1), filters=20, kernel_size=(2, 2, 1)):\n",
    "    with tf.name_scope(f\"decoder_block_{name}\"):\n",
    "        # Upsample the input with a transposed convolution\n",
    "        up = tf.keras.layers.Conv3DTranspose(filters=filters,\n",
    "                                             kernel_size=kernel_size,\n",
    "                                             strides=upscale,\n",
    "                                             padding='SAME',\n",
    "                                             name=f\"upsample_{name}\",\n",
    "                                             use_bias=False)(inputs)\n",
    "        # Concatenate the upsampled input with the other input `x`\n",
    "        up = tf.concat([up, x], axis=-1, name=f\"merge_{name}\")\n",
    "    return up\n",
    "\n",
    "def generator():\n",
    "    \"\"\"\n",
    "    Function to define the generator model of the CNN\n",
    "    \n",
    "    Returns:\n",
    "    model (tf.keras.Model): The generator model\n",
    "    \"\"\"\n",
    "    # Input layer with specified shape\n",
    "    input_ = tf.keras.layers.Input(shape=[height_crop_size,width_crop_size,slice_size,1])\n",
    "    \n",
    "    # Encoder layers with specified iterations and name\n",
    "    conv1, pool1 = encoder_layer(input_, iterations=2, name=\"encode_im1\", training=True, pool=True)\n",
    "    conv2, pool2 = encoder_layer(pool1, iterations=4, name=\"encode_im2\", training=True, pool=True)\n",
    "    conv3, pool3 = encoder_layer(pool2, iterations=6, name=\"encode_im3\", training=True, pool=True)\n",
    "    conv4 = encoder_layer(pool3, iterations=8, name=\"encode_im4\", training=True, pool=False)\n",
    "    \n",
    "    # Decoder layers with specified name\n",
    "    up1 = decoder_layer(conv4, conv3, 10, name=12)\n",
    "    conv7 = encoder_layer(up1, iterations=6, name=\"conv_im6\", training=True, pool=False)\n",
    "    up2 = decoder_layer(conv7, conv2, 8, name=21)\n",
    "    conv8 = encoder_layer(up2, iterations=4, name=\"encode_im7\", training=True, pool=False)\n",
    "    up3 = decoder_layer(conv8, conv1, 6, name=32)\n",
    "    conv9 = encoder_layer(up3, iterations=2, name=\"encode_im8\", training=True, pool=False)\n",
    "    \n",
    "    # Final Conv3D layer\n",
    "    conv10 = tf.keras.layers.Conv3D(1, (1,1,1), name='logits_re_im', padding='SAME')(conv9)\n",
    "    \n",
    "    # Model definition\n",
    "    model = tf.keras.Model(inputs=input_, outputs=conv10)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# disciminator downsampler\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    \"\"\"\n",
    "    Function to create a downsampling layer in the generator network\n",
    "    \n",
    "    Parameters:\n",
    "    filters (int): Number of filters in the Conv3D layer\n",
    "    size (int): The size of the Conv3D layer\n",
    "    apply_batchnorm (bool, optional): If True, adds BatchNormalization layer after Conv3D. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "    result (tf.keras.Sequential): The downsampling layer\n",
    "    \"\"\"\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    result = tf.keras.Sequential()\n",
    "    \n",
    "    # Conv3D layer with specified number of filters and kernel size\n",
    "    result.add(tf.keras.layers.Conv3D(filters, kernel_size=[3,3,3], padding='same',\n",
    "                                      kernel_initializer=initializer))\n",
    "    \n",
    "    # Optional BatchNormalization layer and leaky ReLU activation\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "        result.add(tf.keras.layers.LeakyReLU())\n",
    "    \n",
    "    return result\n",
    "\n",
    "def discriminator():\n",
    "    # Initialize a random normal initializer with mean 0 and standard deviation 0.02\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    \n",
    "    # Create an input layer for the input image with shape [height_crop_size, width_crop_size, slice_size, 1]\n",
    "    inp = tf.keras.layers.Input(shape=[height_crop_size, width_crop_size, slice_size, 1],\n",
    "                                name='input_image')\n",
    "    \n",
    "    # Create an input layer for the target image with shape [height_crop_size, width_crop_size, slice_size, 1]\n",
    "    tar = tf.keras.layers.Input(shape=[height_crop_size, width_crop_size, slice_size, 1],\n",
    "                                name='target_image')\n",
    "    \n",
    "    # Concatenate the input and target images\n",
    "    x = tf.keras.layers.concatenate([inp, tar])\n",
    "    \n",
    "    # Apply the downsample function with 16 filters and a kernel size of 2, and no batch normalization\n",
    "    down1 = downsample(16, 2, False)(x)\n",
    "    \n",
    "    # Apply the downsample function with 32 filters and a kernel size of 2, and batch normalization\n",
    "    down2 = downsample(32, 2)(down1)\n",
    "    \n",
    "    # Apply the downsample function with 64 filters and a kernel size of 2, and batch normalization\n",
    "    down3 = downsample(64, 2)(down2)\n",
    "    \n",
    "    # Apply a zero padding layer to down3\n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding3D()(down3)\n",
    "    \n",
    "    # Apply a Conv3D layer with 128 filters and a kernel size of 4, and stride of 1\n",
    "    conv = tf.keras.layers.Conv3D(128, 4, strides=1, padding='SAME')(zero_pad1)\n",
    "    \n",
    "    # Apply a batch normalization layer to the output of the Conv3D layer\n",
    "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "    \n",
    "    # Apply a leaky ReLU activation layer to the output of the batch normalization layer\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "    \n",
    "    # Apply a zero padding layer to the output of the leaky ReLU activation layer\n",
    "    zero_pad2 = tf.keras.layers.ZeroPadding3D()(leaky_relu)\n",
    "    \n",
    "    # Apply a Conv3D layer with 1 filter and a kernel size of 4, and stride of 1\n",
    "    last = tf.keras.layers.Conv3D(1, 4, strides=1, padding='SAME')(zero_pad2)\n",
    "    \n",
    "    # Return a Model with input layers [inp, tar] and output layer `last`\n",
    "    return tf.keras.Model(inputs=[inp, tar], outputs=last)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9714f169",
   "metadata": {},
   "source": [
    "## Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50993a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(disc, gen, anatomy, flow, gen_optimizer, disc_optimizer):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:  \n",
    "        # Normalize anatomy data between 0 and 1\n",
    "        anatomy = tf.keras.utils.normalize(anatomy)\n",
    "\n",
    "        # Generate anatomy2flow\n",
    "        anatomy2flow = gen(anatomy) \n",
    "\n",
    "        # Calculate magnitude of anatomy2flow\n",
    "        mag = tf.abs(tf.squeeze(anatomy2flow))\n",
    "\n",
    "        # Loss functions\n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "        loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "        # Evaluate discriminator on real data and generated data\n",
    "        discriminator_real_output = disc([anatomy, flow], training=True)\n",
    "        discriminator_generated_output = disc([anatomy, anatomy2flow], training=True)\n",
    "\n",
    "        # Calculate real loss and generated loss for discriminator\n",
    "        real_loss = loss_object(tf.ones_like(discriminator_real_output), discriminator_real_output)\n",
    "        generated_loss = loss_object(tf.zeros_like(discriminator_generated_output), discriminator_generated_output)\n",
    "        total_discriminator_loss = real_loss + generated_loss\n",
    "\n",
    "        # Calculate GAN loss\n",
    "        gan_loss = loss_object(tf.ones_like(discriminator_generated_output), discriminator_generated_output)\n",
    "\n",
    "        # Calculate gradients and laplacian for both magnitude and flow\n",
    "        flow = tf.squeeze(flow)\n",
    "        ai_grady = np.gradient(mag[...].numpy())\n",
    "        man_grady = np.gradient(flow[...].numpy())\n",
    "        ai_grady2 = scipy.ndimage.laplace(mag.numpy())\n",
    "        man_grady2 = scipy.ndimage.laplace(flow.numpy())\n",
    "\n",
    "        # Calculate structural similarity (SSIM)\n",
    "        loss_ssimy = tf.reduce_mean(tf.image.ssim(mag[...]/tf.reduce_max(mag[...]), \\\n",
    "                                                 flow[...]/tf.reduce_max(flow[...]),1.0))\n",
    "\n",
    "        # Calculate final loss function\n",
    "        loss_fn = gan_loss + 1000 * mse(mag, flow) + (1-loss_ssimy) + 1000 * mse(ai_grady, man_grady) + 10 * mse(ai_grady2, man_grady2)\n",
    "    \n",
    "    # Trainable variables for discriminator and generator\n",
    "    variables1 = disc.trainable_variables     \n",
    "    variables = gen.trainable_variables \n",
    "    \n",
    "    # Calculate gradients for generator and discriminator\n",
    "    gradients = gen_tape.gradient(loss_fn, variables)\n",
    "    gradients1 = disc_tape.gradient(total_discriminator_loss, variables1)\n",
    "    \n",
    "    # Apply gradients using optimizers\n",
    "    gen_optimizer.apply_gradients(zip(gradients,variables)) \n",
    "    disc_optimizer.apply_gradients(zip(gradients1,variables1))\n",
    "    return loss_fn, loss_ssimy , gan_loss\n",
    "\n",
    "# define LinearDecay schedular\n",
    "class LinearDecay(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_learning_rate, total_steps, step_decay):\n",
    "        super(LinearDecay, self).__init__()\n",
    "        self._initial_learning_rate = initial_learning_rate\n",
    "        self._steps = total_steps\n",
    "        self._step_decay = step_decay\n",
    "        self.current_learning_rate = tf.Variable(initial_value=initial_learning_rate,\\\n",
    "                                                 trainable=False, dtype=tf.float32)\n",
    "    def __call__(self, step):\n",
    "        self.current_learning_rate.assign(tf.cond(\n",
    "            step >= self._step_decay,\n",
    "            true_fn=lambda: self._initial_learning_rate * (1 - 1 /  (self._steps - self._step_decay) \\\n",
    "                                                           *(step - self._step_decay)),\n",
    "            false_fn=lambda: self._initial_learning_rate\n",
    "        ))\n",
    "        return self.current_learning_rate\n",
    "\n",
    "# define the generator and discriminator optimizers\n",
    "llr = LinearDecay(0.0002, epochs*len_dataset, epoch_decay*len_dataset)\n",
    "gen_optimizer = tf.keras.optimizers.Adam(learning_rate=llr) \n",
    "disc_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e45301",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "This code is checking if the output directory and checkpoint directory exists, if not it creates them. Then it defines the generator and discriminator models, the checkpoint prefix, and checkpoint object. Finally, it checks if a checkpoint exists, and if so, it restores the model from the latest checkpoint by using the ``tf.train.latest_checkpoint`` method. The number of trained epochs is also found by parsing the checkpoint address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d694880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Outer Epoch:   0%|                                                                                                                                   | 0/40 [00:00<?, ?it/s]\n",
      "Inner Epoch:   0%|                                                                                                                                 | 0/1095 [00:00<?, ?it/s]\u001b[A2023-02-07 10:59:12.538762: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 476 of 1095\n",
      "2023-02-07 10:59:22.535487: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 1006 of 1095\n",
      "2023-02-07 10:59:24.182266: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:417] Shuffle buffer filled.\n",
      "2023-02-07 10:59:28.237208: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n",
      "2023-02-07 10:59:34.110721: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:85] Couldn't get ptxas version string: INTERNAL: Running ptxas --version returned 32512\n",
      "2023-02-07 10:59:34.170640: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:318] INTERNAL: ptxas exited with non-zero error code 32512, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2023-02-07 10:59:36.367166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-02-07 10:59:41.754706: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x55db23ac4f30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-02-07 10:59:41.754756: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0\n",
      "2023-02-07 10:59:42.284852: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-02-07 10:59:42.606885: W tensorflow/compiler/xla/service/gpu/nvptx_helper.cc:56] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  /usr/local/cuda-11.2\n",
      "  /usr/local/cuda\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n",
      "2023-02-07 10:59:42.756878: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2023-02-07 10:59:42.757833: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2023-02-07 10:59:42.758277: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "Inner Epoch:   0%|                                                                                                                                 | 0/1095 [00:41<?, ?it/s]\n",
      "Outer Epoch:   0%|                                                                                                                                   | 0/40 [00:41<?, ?it/s]\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "libdevice not found at ./libdevice.10.bc [Op:__inference__update_step_xla_8427]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m     flow \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(flow,(\u001b[38;5;241m1\u001b[39m,height_crop_size,width_crop_size,slice_size,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Train the generator and discriminator models\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     loss, sss, gn \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisc\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43manatomy\u001b[49m\u001b[43m,\u001b[49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisc_optimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Save the model checkpoint after each outer epoch\u001b[39;00m\n\u001b[1;32m     48\u001b[0m checkpoint\u001b[38;5;241m.\u001b[39msave(file_prefix\u001b[38;5;241m=\u001b[39mcheckpoint_prefix)\n",
      "Cell \u001b[0;32mIn[5], line 51\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(disc, gen, anatomy, flow, gen_optimizer, disc_optimizer)\u001b[0m\n\u001b[1;32m     48\u001b[0m gradients1 \u001b[38;5;241m=\u001b[39m disc_tape\u001b[38;5;241m.\u001b[39mgradient(total_discriminator_loss, variables1)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Apply gradients using optimizers\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[43mgen_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     52\u001b[0m disc_optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(gradients1,variables1))\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_fn, loss_ssimy , gan_loss\n",
      "File \u001b[0;32m~/.conda/envs/tensorflow-2.6-py38/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1140\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_gradients_aggregation \u001b[38;5;129;01mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[1;32m   1139\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_gradients(grads_and_vars)\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tensorflow-2.6-py38/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:634\u001b[0m, in \u001b[0;36m_BaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    633\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables))\n\u001b[0;32m--> 634\u001b[0m iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[0;32m~/.conda/envs/tensorflow-2.6-py38/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1166\u001b[0m, in \u001b[0;36mOptimizer._internal_apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_internal_apply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[0;32m-> 1166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_merge_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_apply_gradients_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribution_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tensorflow-2.6-py38/lib/python3.10/site-packages/tensorflow/python/distribute/merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[0;32m---> 51\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m distribution_strategy_context\u001b[38;5;241m.\u001b[39mget_replica_context()\u001b[38;5;241m.\u001b[39mmerge_call(\n\u001b[1;32m     54\u001b[0m       fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/tensorflow-2.6-py38/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1216\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn\u001b[0;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step(grad, var)\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[0;32m-> 1216\u001b[0m     \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_grad_to_update_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[1;32m   1221\u001b[0m     _, var_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n",
      "File \u001b[0;32m~/.conda/envs/tensorflow-2.6-py38/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:2637\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2634\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   2635\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   2636\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 2637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2638\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2639\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[1;32m   2640\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[0;32m~/.conda/envs/tensorflow-2.6-py38/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:3710\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[1;32m   3708\u001b[0m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[1;32m   3709\u001b[0m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[0;32m-> 3710\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_non_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tensorflow-2.6-py38/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:3716\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   3712\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[1;32m   3713\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[1;32m   3714\u001b[0m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[1;32m   3715\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[0;32m-> 3716\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[1;32m   3718\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/tensorflow-2.6-py38/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:595\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    594\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tensorflow-2.6-py38/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1211\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_grad_to_update_var\u001b[39m(var, grad):\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit_compile:\n\u001b[0;32m-> 1211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_step_xla\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_var_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step(grad, var)\n",
      "File \u001b[0;32m~/.conda/envs/tensorflow-2.6-py38/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.conda/envs/tensorflow-2.6-py38/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInternalError\u001b[0m: libdevice not found at ./libdevice.10.bc [Op:__inference__update_step_xla_8427]"
     ]
    }
   ],
   "source": [
    "# Define the output directory\n",
    "output_dir = './haben_output'\n",
    "\n",
    "# If the output directory does not exist, create it\n",
    "if not tf.io.gfile.isdir(output_dir):\n",
    "    tf.io.gfile.mkdir(output_dir)\n",
    "\n",
    "# Define the generator and discriminator models\n",
    "gen = generator()\n",
    "disc = discriminator()\n",
    "\n",
    "# Define the checkpoint directory\n",
    "checkpoint_dir = tf.io.gfile.join(output_dir, 'training_checkpoints')\n",
    "\n",
    "# If the checkpoint directory does not exist, create it\n",
    "if not tf.io.gfile.isdir(checkpoint_dir):\n",
    "    tf.io.gfile.mkdir(checkpoint_dir)\n",
    "\n",
    "# Define the checkpoint prefix and the checkpoint object\n",
    "checkpoint_prefix = tf.io.gfile.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(gen_optimizer=gen_optimizer,\n",
    "                                 disc_optimizer=disc_optimizer,\n",
    "                                 gen=gen,\n",
    "                                 disc=disc)\n",
    "\n",
    "# Get the address of the latest checkpoint\n",
    "checkpoint_address = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "# If a checkpoint exists, restore the model from it\n",
    "epochs_so_far = 0\n",
    "if checkpoint_address:\n",
    "    checkpoint.restore(checkpoint_address)\n",
    "    hyphen = checkpoint_address.index('-')\n",
    "    epochs_so_far = int(checkpoint_address[hyphen+1:])\n",
    "    print(\"Restored the model from epoch {}\".format(epochs_so_far))\n",
    "\n",
    "\n",
    "# Train the model for each outer epoch\n",
    "for epoch in tqdm.trange(epochs_so_far+1,epochs+1,desc=\"Outer Epoch\",total=epochs-epochs_so_far):\n",
    "    # Train the model for each inner epoch\n",
    "    for anatomy,flow in tqdm.tqdm(dataset_train,desc=\"Inner Epoch\",total=len_dataset):\n",
    "        # Reshape the anatomy and flow data to a specific shape\n",
    "        anatomy = tf.reshape(anatomy,(1,height_crop_size,width_crop_size,slice_size,1))\n",
    "        flow = tf.reshape(flow,(1,height_crop_size,width_crop_size,slice_size,1))\n",
    "        # Train the generator and discriminator models\n",
    "        loss, sss, gn = train_step(disc,gen,anatomy,flow, gen_optimizer, disc_optimizer)\n",
    "    # Save the model checkpoint after each outer epoch\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow-2.6-py38)",
   "language": "python",
   "name": "tensorflow-2.6-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
